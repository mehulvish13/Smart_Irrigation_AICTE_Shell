{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "947ef530",
   "metadata": {},
   "source": [
    "## Importing Required Libraries\n",
    "This section imports all necessary libraries for data manipulation, visualization, machine learning model development, and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6dad0cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd# Importing pandas for data manipulation\n",
    "import matplotlib.pyplot as plt# Importing matplotlib for plotting\n",
    "import seaborn as sns# Importing seaborn for enhanced visualization\n",
    "from sklearn.model_selection import train_test_split# Importing train_test_split for splitting the dataset\n",
    "from sklearn.ensemble import RandomForestClassifier# Importing RandomForestClassifier for classification tasks\n",
    "from sklearn.multioutput import MultiOutputClassifier# Importing MultiOutputClassifier for multi-label classification\n",
    "from sklearn.metrics import classification_report# Importing classification_report for evaluating the model\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler# Importing MinMaxScaler for feature scaling\n",
    "import joblib# Importing joblib for saving and loading models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e07adb1",
   "metadata": {},
   "source": [
    "## Loading the Dataset\n",
    "The dataset is loaded into a pandas DataFrame named `df` for further analysis and processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee24b8f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(\"irrigation_machine.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "252125de",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()#print first 5 rows of the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad112a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail()#print last 5 rows of the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "118b48c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()#print information about the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c897b575",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns#give name of all name of columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6154e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop('Unnamed: 0', axis=1)\n",
    "df.head() # # print print first first 5 5 rows rows of of the the dataframe dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75a6d8c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()#print statistical summary of the dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f46af3a6",
   "metadata": {},
   "source": [
    "## Defining Features and Labels\n",
    "In this step, the independent variables (features) and dependent variables (labels) are separated for use in model training and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9492c682",
   "metadata": {},
   "outputs": [],
   "source": [
    "x=df.iloc[:, 0:20]#give name of all columns from 0 to 20 as \n",
    "#x=independent variable\n",
    "y=df.iloc[:, 20:]#dependent variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f635f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "x.sample(10)#print random n rows of the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa1dff90",
   "metadata": {},
   "outputs": [],
   "source": [
    "y.sample(10)#print random n rows of the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef8b64ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "x.info()#print information about the independent variable dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "764d3b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "y.info()#print information about the dependent variable dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37dbd700",
   "metadata": {},
   "outputs": [],
   "source": [
    "x#print independent variable dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d15793c",
   "metadata": {},
   "outputs": [],
   "source": [
    "x.shape,y.shape  #give shape of independent and dependent variable dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96416224",
   "metadata": {},
   "source": [
    "## Feature Scaling\n",
    "Feature scaling is performed to normalize all input features to a common scale, typically between 0 and 1. This process enhances model performance, ensures balanced feature contribution, and improves numerical stability during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37a4b1de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a MinMaxScaler object to normalize the features\n",
    "scalar = MinMaxScaler()\n",
    "\n",
    "# Fit the scaler to the data and transform it\n",
    "# fit: learns the min/max values of features\n",
    "# transform: applies the scaling using formula (x - min)/(max - min)\n",
    "x_scaled = scalar.fit_transform(x)\n",
    "\n",
    "# Display the scaled features where:\n",
    "# - All values are now between 0 and 1\n",
    "# - Helps prevent features with larger ranges from dominating the model\n",
    "# - Important for model performance and stability\n",
    "x_scaled\n",
    "#output is a numpy 2d array with normalized values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42314b91",
   "metadata": {},
   "source": [
    "## Splitting the Dataset: Training and Testing Sets\n",
    "The dataset is divided into training and testing subsets to evaluate the model's generalization performance. Typically, 80% of the data is used for training and 20% for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9f5b2db",
   "metadata": {},
   "outputs": [],
   "source": [
    "#split the data into training and testing sets\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_scaled, y, test_size=0.2, random_state=42)\n",
    "#test size=0.2 means 20% of the data will be used for testing, random_state=42 ensures reproducibility\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14f5d1ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.shape, x_test.shape, y_train.shape, y_test.shape#give shape of training and testing sets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fe0eb9a",
   "metadata": {},
   "source": [
    "## Model Training: Classifier Development\n",
    "A Random Forest classifier is trained using the training data. MultiOutputClassifier is used to enable multi-label classification, allowing the model to predict irrigation needs for multiple parcels simultaneously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6743eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use MultiOutputClassifier to handle multi-label classification\n",
    "from sklearn.ensemble import RandomForestClassifier # Importing RandomForestClassifier for classification tasks\n",
    "from sklearn.multioutput import MultiOutputClassifier   # Importing MultiOutputClassifier for multi-label classification\n",
    "\n",
    "# Custom hyperparameters for RandomForest\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=200,         # Number of trees\n",
    "    max_depth=10,             # Maximum depth of each tree\n",
    "    min_samples_split=4,      # Minimum samples to split a node\n",
    "    min_samples_leaf=2,       # Minimum samples per leaf\n",
    "    max_features='sqrt',      # Number of features to consider at each split ('auto', 'sqrt', 'log2', or int)\n",
    "    random_state=42 # For reproducibility\n",
    ")\n",
    "\n",
    "# Wrap it with MultiOutputClassifier\n",
    "model=MultiOutputClassifier(rf) # Using MultiOutputClassifier to handle multi-label classification\n",
    "# Train the model\n",
    "model.fit(x_train, y_train) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc44cdd0",
   "metadata": {},
   "source": [
    "## Model Evaluation\n",
    "The trained model is evaluated on the test set using classification metrics to assess its predictive performance and reliability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49b7803d",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred=model.predict(x_test) # Predict on the test set\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred)) # Print classification report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "158e3862",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df[['parcel_0', 'parcel_1', 'parcel_2']].sum())   # Print the sum of each parcel's irrigation needs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "558a7eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt  # Import library for creating plots\n",
    "\n",
    "# Dictionary containing different irrigation scenarios\n",
    "conditions = {\n",
    "    # Single parcel operations\n",
    "    \"Parcel 0 ON\": df['parcel_0'],          # Status of first parcel\n",
    "    \"Parcel 1 ON\": df['parcel_1'],          # Status of second parcel\n",
    "    \"Parcel 2 ON\": df['parcel_2'],          # Status of third parcel\n",
    "    \n",
    "    # Combined operations (using & for logical AND)\n",
    "    \"Parcel 0 & 1 ON\": df['parcel_0'] & df['parcel_1'],    # When both parcel 0 and 1 are active\n",
    "    \"Parcel 0 & 2 ON\": df['parcel_0'] & df['parcel_2'],    # When both parcel 0 and 2 are active\n",
    "    \"Parcel 1 & 2 ON\": df['parcel_1'] & df['parcel_2'],    # When both parcel 1 and 2 are active\n",
    "    \"All Parcels ON\": df['parcel_0'] & df['parcel_1'] & df['parcel_2'],  # When all parcels are active\n",
    "}\n",
    "\n",
    "# Create a figure with multiple subplots stacked vertically\n",
    "# nrows=len(conditions): Create as many rows as conditions\n",
    "# figsize=(10,15): Set figure size width=10, height=15\n",
    "# sharex=True: All subplots share the same x-axis\n",
    "fig, axs = plt.subplots(nrows=len(conditions), figsize=(10,15), sharex=True)\n",
    "\n",
    "# Create plots for each condition\n",
    "for ax, (title, condition) in zip(axs, conditions.items()):\n",
    "    # ax.step: Create a step plot (good for ON/OFF data)\n",
    "    # df.index: X-axis values (time points)\n",
    "    # condition.astype(int): Convert boolean to 0/1\n",
    "    # where='post': Step happens after the data point\n",
    "    ax.step(df.index, condition.astype(int), where='post', linewidth=1, color='teal')\n",
    "    \n",
    "    # Set title and labels for each subplot\n",
    "    ax.set_title(f\"Sprinkler - {title}\")     # Add title to subplot\n",
    "    ax.set_ylabel(\"Status\")                   # Label y-axis\n",
    "    ax.set_yticks([0, 1])                    # Set y-axis tick positions\n",
    "    ax.set_yticklabels(['OFF', 'ON'])        # Label the ticks\n",
    "\n",
    "# Add x-axis label to the bottom subplot\n",
    "axs[-1].set_xlabel(\"Time Index (Row Number)\")\n",
    "\n",
    "# Display all the plots\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22797b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate combined activity of all pumps (overlap)\n",
    "any_pump_on = (df['parcel_0'] == 1) | (df['parcel_1'] == 1) | (df['parcel_2'] == 1)\n",
    "\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "# Plot individual pump statuses\n",
    "plt.step(df.index, df['parcel_0'], where='post', linewidth=2, label='Parcel 0 Pump', color='blue')\n",
    "plt.step(df.index, df['parcel_1'], where='post', linewidth=2, label='Parcel 1 Pump', color='orange')\n",
    "plt.step(df.index, df['parcel_2'], where='post', linewidth=2, label='Parcel 2 Pump', color='green')\n",
    "\n",
    "plt.title(\"Pump Activity and Combined Farm Coverage\")\n",
    "plt.xlabel(\"Time Index (Row Number)\")\n",
    "plt.ylabel(\"Status\")\n",
    "plt.yticks([0, 1], ['OFF', 'ON'])\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6f380f9",
   "metadata": {},
   "source": [
    "## Machine Learning Model Workflow Steps\n",
    "This notebook follows a structured approach to develop a machine learning model for smart irrigation. The key steps are:\n",
    "1. **Data Collection**: Gather relevant data for the irrigation system.\n",
    "2. **Data Loading**: Import the dataset into the working environment for analysis.\n",
    "3. **Exploratory Data Analysis (EDA)**: Analyze and visualize the data to understand patterns, trends, and relationships.\n",
    "4. **Model Building**: Develop and train the machine learning model using appropriate algorithms and techniques.\n",
    "5. **Model Evaluation**: Assess the model's performance using suitable metrics and validation methods.\n",
    "6. **Model Saving**: Save the trained model for future deployment and inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f3c77b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "joblib.dump(model, \"Farm_Irrigation_System.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b848241b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
